\documentclass{beamer}
\usepackage[T1]{fontenc}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[italian]{babel}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{quoting}
\quotingsetup{font=small}
\usepackage{diagbox}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{float}
\usepackage{version}
\usepackage{multicol}
\usepackage{beamerfoils}

\theoremstyle{plain}
\newtheorem{Proposizione}{Proposizione}
\newtheorem{Corollario}{Corollario}
\newtheorem*{Definizione}{Definizione}
\newtheorem{Teorema}{Teorema}
%\newtheorem{Lemma}{Lemma}

%\def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits}

%\newcommand{\eqdef}{\stackrel{\mathrm{def}}{:=}}

\usepackage{appendixnumberbeamer}
\usetheme{Ilmenau}
\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{navigation symbols}{}
\setbeamercovered{highly dynamic}

\title{\LARGE{\textbf{Progetto bayesian}}}
\author{\texorpdfstring{Francesca Di Filippo  Elena Musiari marta enrico gabriele}{Candidato}}
\institute[Polimi]{Bayesian Statistics\\{Politecnico di Milano}\\A.A.2021-2022}
%\institute{Universit\`a degli Studi di Parma}
%\date{24 Luglio 2020} 

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Struttura dell'elaborato}
\tableofcontents
\end{frame}

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 

\begin{section}{ABC}
\begin{frame}{Initial problem}
	Problem: computationally prohibitive (or unavailable) evaluation of the likelihood function
	
	\vspace{1cm}
	Given the posterior distribution $\pi(\theta | y_{obs}) = \frac{p(y_{obs}|\theta) \pi(\theta)}{\int _{\Theta }p(y_{obs}|\theta) \pi(\theta) d\theta }$ 
\end{frame}

\begin{frame}{Approach}
	To solve this issue we can use methods based on the approximation of the likelihood function, called \textit{Likelihood-free methods}.
	\vspace{0.5cm}
	
	algo generale
	
	\vspace{0.5cm}
	%Balance: avoid the likelihood evaluation VS computational inefficiency
	Is this an efficient method for complex analysis?

\end{frame}

\begin{frame}{Likelihood-free}
	punto 3 modificato  < h
	
	\vspace{0.5cm}
	
	This adjust the potentially very low (or zero) probability requirement that $y = y_{obs}$ exactly.
\end{frame}

\begin{frame}{ABC}
	We focused on a particular case of the Likelihood-free methods: the \emph{Approximate Bayesian Computation (ABC)}.
	
	\vspace{0.5cm}
	
	The aim: find a practical way of performing Bayesian analysis, while keeping the approximation and the computation to a minimum.
	
	\vspace{0.5cm}
	The likelihood-free rejection algorithm is sampling from the joint distribution $\propto \mathbb{I}(\parallel y-y_{obs} \parallel \leq h)p(y|\theta)\pi(\theta)$
	
	
	$\Longrightarrow$ replace the indicator function with a standard smoothing kernel function $K_h(u)$, with $u=\parallel y-y_{obs} \parallel$:
	\[ K_h(u)  =  \frac{1}{h}  K  ( \frac{u}{h} )  \]
	
	
	\vspace{0.2cm}
	Hence:
	\[\pi_{ABC}(\theta, y | y_{obs}) \propto K_h(u)p(y|\theta)\pi(\theta)\]
	
	
\end{frame}

\begin{frame}{ABC - summary statistics}
	Is this feasible in practice?
	
	\vspace{0.5cm}
	In practice: difficult to have $y\approx y_{obs}$ from $p(y|\theta)$, unless $y_{obs}$ very low dimensional or $p(y|\theta)$ factorises into low-dimensional components.
	
	Thus we should use a large $h$, obtaining a poor posterior approximation!
	
	$\Longrightarrow$ use summary statistics $s = S(y)$
	
	\[  \pi_{ABC} (\theta | s_{obs})   \]
\end{frame}

\begin{frame}{Summary statistics}

	Critical decision: choice of summary statistics
	
	\vspace{0.3cm}
	Dimension of summary statistics:
	\begin{itemize}
		\item large enough to contain as much as informations about observed data as possible
		\item low enough to avoid curse of dimensionality of matching $s$ and $s_{obs}$
	\end{itemize}

	\vspace{0.3cm}
	$\Longrightarrow$ choose sufficient statistics, such that:
	\[   \pi(\theta|s_{obs}) \equiv \pi(\theta|y_{obs})   \]

\end{frame}

\begin{frame}{Distance measure}
	Distance measure: substantial impact on ABC algorithm efficiency
	
	\[  \parallel s - s_{obs} \parallel = (s - s_{obs})^\top \Sigma^{-1} (s - s_{obs}) \]
	
	\begin{itemize}
		\item $\Sigma = $ identity matrix $ \rightarrow$ Euclidean distance 
		\item $\Sigma =$ diagonal matrix of non-zero weights $ \rightarrow$ Weighted Euclidean distance 
		\item $\Sigma = $ full covariance matrix of $s \rightarrow$ Mahalanobis distance 
	\end{itemize}
\end{frame}

\begin{frame}{ABC algorithm}
	algoritmo pag 25-26
	
	we can add a stopping rule to the ABC Rejection Sampling Algorithm
\end{frame}
	
\end{section}






\begin{frame}
\vspace{1.1cm}
\centering
{\LARGE Grazie per l'attenzione!}
\end{frame}


\end{document}