\documentclass{beamer}
%pacchetti
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[italian]{babel}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{quoting}
\quotingsetup{font=small}
\usepackage{diagbox}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{float}
\usepackage{version}
\usepackage{multicol}
\usepackage{beamerfoils}

\usepackage[none]{hyphenat} %avoid hyphenation
\usepackage{xcolor} %to uset \textcolor
% end pacchetti

\usetheme[bgphoto]{polimi}

% Full instructions available at:
% https://github.com/elauksap/beamerthemepolimi

% Set custom font (requires to compile with XeLaTeX).
\usepackage{ifxetex}
\ifxetex
\usepackage{fontspec}
\setsansfont[Scale=0.9]{Arial}
\fi

\usepackage{lipsum}


\newcommand\mynum[1]{%
	\usebeamercolor{enumerate item}%
	\tikzset{beameritem/.style={circle,inner sep=0,minimum size=2ex,text=enumerate item.bg,fill=enumerate item.fg,font=\footnotesize}}%
	\tikz[baseline=(n.base)]\node(n)[beameritem]{#1};%
}


\title{Coupled Markov chains with applications to Approximate Bayesian Computation for model based clustering}
%\subtitle{Subtitle}
\author{E. Bertoni, M. Caldarini, F. Di Filippo, G. Gabrielli, E. Musiari}
\date{11 november 2021}


%Cose da fare

% Mettere biografia
% Mettere \pause e colori
% Sistemare il titolo e layout

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\begin{section}{Introduction}

	\begin{frame}
		\frametitle{A complex problem}
 		
 		Our goal is to propose a method which solves these two problems:
 		
 		\begin{itemize}
 			\item find a way to speed up computation time by parallelizing a Monte Carlo Markov chain method;
 			
 			\onslide<2->{$\Longrightarrow$ Unbiased Markov chain Monte Carlo methods with couplings;}
 			
 			\item tackle the case in which the likelihood function has a computationally intractable evaluation. 
 			
 			\onslide<3->{$\Longrightarrow$ Approximate Bayesian Computation}
 		\end{itemize}
	\end{frame}

\end{section}

\begin{section}{Unbiased Markov chain Monte Carlo methods with couplings}
	\begin{frame}[plain]{}
		\sectionpage
	\end{frame}

	\begin{frame}
	 	\frametitle{The road to parallelization: coupling of Markov chains}
		In order to parallelize we need an \textbf{unbiased} estimator.
		\vspace{0.3 cm }
		Standard Markov chain Monte Carlo methods are potentially biased for any fixed number of iterations.\\
		\vspace{0.3 cm }
		P. Glynn and C. Rhee proposed in 2018 the class of exact estimations algorithms using coupling of Markov chain.\\
		\vspace{0.3 cm }
		\pause
 		\textit{What is the coupling of Markov chains?}
 		The coupling of two probability distributions $\mu$ and $\nu$ refers to the construction of a bivariate probability distribution whose marginals are the original distributions $\mu$ and $\nu$.
 		\vspace{0.3 cm }
 		Markov chain coupling allow to \textbf{reduce the convergence time}.
	\end{frame}

	\begin{frame}
	 	\frametitle{Rhee--Glynn estimator I}
	 	The goal is to estimate
	 	$$
	 	\mathbb{E}_{\pi}[h(X)] 
	 	= \int h(x) \pi (\text{d}x)
	 	.
	 	$$
	 	The estimator we are going to construct is based on a coupled pair of Markov chains, $(X_t)_{t\geq 0}$ and $(Y_t)_{t\geq}$, which marginally start from $\pi_0$ and evolve accordingly to $P$.\\ 
	\end{frame}
	
	\begin{frame}
	 	We consider some assumptions:
	 	\begin{enumerate}
	 		\item as $t \to \infty$, 
	 		$$ \mathbb E [h(X_t)] \to \mathbb E_\pi [h(X)];$$
	 		and there exists $\eta > 0$ and $D < \infty$ such that $\mathbb E [|h(X_t)|^{2 + \eta}] \leq D$ for all $t \geq 0$;
	 		
	 		\pause
	 		\item the chains are such that the meeting time 
	 		$$
	 			\tau 
	 			= \inf\{t \geq 1 : X_t = Y_{t-1}\}
	 		$$ 
	 		satisfies $\mathbb{P}(\tau > t) \leq C \delta^t$ for all $t \geq 0$, for some constants $C < \infty$ and $\delta \in (0,1)$;
	 		
	 		\pause
	 		\item the chains stay together after meeting:
	 		$$X_t = Y_{t-1} \text{ for all } t \geq \tau.$$
	 	\end{enumerate}
	\end{frame}
	
	\begin{frame}
	 	\frametitle{Rhee--Glynn estimator II}
	 	Thanks to the previous assumptions we can prove that:
	 	\begin{align*}
	 	\mathbb{E}_{\pi}[h(X)] = \mathbb{E}[
	 		\textcolor{orange}{h(X_k) + \sum_{t = k+1}^{\tau -1}\{h(X_t) - h(Y_{t-1})\} } ]
	 	;
	 	\end{align*}
	 	and we define the Rhee--Glynn estimator as:
	 	$$ 
	 		H_k(X,Y)
	 		= h(X_k) + \sum_{t = k+1}^{\tau -1}\{h(X_t) - h(Y_{t-1})\}
	 	$$
	 	which is \textbf{unbiased} by construction.
	\end{frame}

	\begin{frame}
	 	\frametitle{Time-averaged estimator}
	 	In such form, Rhee-Glynn is not computationally feasible, the time-averaged estimator keeps the assumptions allowing the computation:
	 	\only<1>{
		 	$$
		 	H_{k:m}
		 	= \frac{1}{m-k+1}\sum_{l=k}^{m}h(X_l) 
		 	+ \sum_{l=k+1}^{\tau -1}\min(1, \frac{l-k}{m-k+1})\{h(X_l)-h(Y_{l-1})\} 
	 		$$
 		}
	 	\only<2,3>{
	 		$$
	 		H_{k:m}
	 		= \underbrace{
	 			\frac{1}{m-k+1}\sum_{l=k}^{m}h(X_l) }_{MCMC_{k:m}}
	 		+ \underbrace{ 
	 			\sum_{l=k+1}^{\tau -1}\min(1, \frac{l-k}{m-k+1})\{h(X_l)-h(Y_{l-1})\} }_{BC_{k:m}}
	 		$$
 		}
	 	
	 	 %m-k deve essere comunque grande, m>> k
	 	\pause
	 	\begin{itemize}
	 		\item $MCMC_{k:m}$  is the standard MCMC average;
	 		\item $BC_{k:m}$ is the bias correction;
	 	\end{itemize}
 		\vspace{0.3cm}
 		\pause
	 	\begin{itemize}
	 		\item $k-1$ number of burn-in iterations;
	 		\item $m$ is a fixed integer of number of maximum iterations;
	 		\item $\tau$ is a random variable representing the meeting time.

	 	\end{itemize}
	 	
	\end{frame}

	\begin{frame}
	 	\frametitle{Application to Metropolis-Hasting algorithm I}
		The MH \textbf{algorithm} adapted with couplings:
	 	\vspace{0.3cm}
	 	\begin{enumerate}
	 		\item draw $X_0$ and $Y_0$ from an initial distribution $\pi_0$ and draw $X_1 \sim P(X_0, \cdot)$;
	 		\item set $t=1$: while $t<\max\{m,\tau\}$ and:
	 		\begin{itemize}
	 			\item[a] draw $(X_{t+1}, Y_t)\sim \bar P \{(X_t, Y_{t-1}), \cdot \}$;
	 			\item[b] set $t \leftarrow t+1$;
	 		\end{itemize}
	 		\item compute 
	 		$$ H_{k:m}(X,Y)$$
	 		with the time-averaged estimator.
	 	\end{enumerate}
	\end{frame}

	\begin{frame} 	
		\frametitle{Application to Metropolis-Hasting algorithm II}
	 	The following is the \textbf{algorithm} to calculate the coupled kernel $\bar P \{(X_t, Y_{t-1}), \cdot \}$ via MH:
	 	\begin{enumerate}
	 		\item sample $(X^\star, Y^\star) | (X_t, Y_{t-1})$ from a maximal coupling of $q(X_t, \cdot)$ and $q(Y_{t-1}, \cdot)$;
	 		\item sample $U \sim \mathcal{U}([0,1])$;
	 		\item if
	 		$$ U
	 		\leq \min\bigg \{
	 		1,
	 		\frac{ \pi(X^\star)q(X^\star,X_t)}{
	 			\pi(X_t)q(X_t, X^\star)}
	 		\bigg \}
	 		$$
	 		then $X_{t+1} = X^\star$; otherwise $X_t = X_{t-1}$;
	 		\item if
	 		$$ U
	 		\leq \min\bigg \{ 
	 		1,
	 		\frac{ \pi(Y^\star)q(Y^\star,Y_t)}{
	 			\pi(Y_t)q(Y_t, Y^\star)}
	 		\bigg \}
	 		$$
	 		then $Y_{t+1} = Y^\star$; otherwise $Y_t = Y_{t-1}$.
	 	\end{enumerate}
	\end{frame}

\end{section}



\begin{section}{Approximate Bayesian Computation}

    \begin{frame}[plain]{}
		\sectionpage
	\end{frame}

	\begin{frame}{Likelihood-free methods I}
		\only<1>{
			To solve this issue we can use methods based on the \textbf{approximation of the likelihood function}, called \textit{Likelihood-free methods}. Here the \textbf{algorithm}:
			\vspace{0.2cm}
			\emph{Inputs:}
			\begin{itemize}
				\item a target posterior density $\pi(\theta | y_{obs}) \propto p(y_{obs}|\theta) \pi(\theta)$, consisting of a prior distribution $\pi(\theta)$ and a procedure of generating data under the model $p(y_{obs}|\theta)$;
				\item a proposal density $g(\theta)$, with $g(\theta)\textgreater 0 $ if $\pi(\theta | y_{obs}) \textgreater 0$;
				\item an integer $N > 0$.
			\end{itemize}
		}
	
		\only<2>{		
			\emph{Sampling} for $i= 1,..., N$:
				\begin{enumerate}
				\item generate $\theta ^ {(i)} \sim g(\theta)$ from sampling density $g$;
				\item generate $ y \sim p(y|\theta ^ {(i)})$ from the likelihood;
				\item if $y=y_{obs}$, then accept $\theta ^ {(i)}$ with probability $\frac{\pi(\theta ^ {(i)})}{K g(\theta ^ {(i)})}$, where $K \geq \max_{\theta}{\frac{\pi(\theta)}{g(\theta)}}$; else go to 1.
			\end{enumerate}
			
			\emph{Output:}
			\begin{itemize}
				\item a set of parameter vectors $\theta ^ {(1)},..., \theta ^ {(N)}$ which are samples from $\pi(\theta |y_{obs})$.
			\end{itemize}
		}

	\end{frame}

	\begin{frame}{Likelihood-free methods II}
		Is this an efficient method for complex analysis?
		
		\pause
		\vspace{0.3cm}
		\mynum{3} If $\parallel y-y_{obs}\parallel \leq h$, then accept $\theta ^ {(i)}$ with probability $\frac{\pi(\theta ^ {(i)})}{K g(\theta ^ {(i)})}$, where $K \geq \max_{\theta}{\frac{\pi(\theta)}{g(\theta)}}$. 
		
		\vspace{0.2cm}
		Else go to 1. 
	\end{frame}

	\begin{frame}{Approximate Bayesian Computation}
		We focused on a particular case of the Likelihood-free methods: the \emph{Approximate Bayesian Computation (ABC)}.
		
		\vspace{0.5cm}
		
		The aim: find a practical way of performing Bayesian analysis, while keeping the approximation and the computation to a minimum.
		
		\vspace{0.5cm}
		The likelihood-free rejection algorithm is sampling from the joint distribution $\propto \mathbb{I}(\parallel y-y_{obs} \parallel \leq h)p(y|\theta)\pi(\theta)$
		
	
		$\Longrightarrow$ replace the indicator function with a standard smoothing kernel function $K_h(u)$, with $u=\parallel y-y_{obs} \parallel$:
		\[ K_h(u)  =  \frac{1}{h}  K  ( \frac{u}{h} )  \]
		
		
		\vspace{0.2cm}
		Hence:
		\[\pi_{ABC}(\theta, y | y_{obs}) \propto K_h(u)p(y|\theta)\pi(\theta)\]

	\end{frame}

	\begin{frame}{Summary statistics I}
		Is this feasible in practice?
		
		\vspace{0.5cm}
		In practice: difficult to have $y\approx y_{obs}$ from $p(y|\theta)$, unless $y_{obs}$ very low dimensional or $p(y|\theta)$ factorises into low-dimensional components.
		
		Thus we should use a large $h$, obtaining a poor posterior approximation!
		
		$\Longrightarrow$ use summary statistics $s = S(y)$
		
		\[  \pi_{ABC} (\theta | s_{obs})   \]
	\end{frame}
		
	\begin{frame}{Summary statistics II}
		
		Critical decision: choice of summary statistics.\\
		
		\vspace{0.3cm}\\
		
		Dimension of summary statistics:
		\begin{itemize}
			\item large enough to contain as much as information about observed data as possible
			\item low enough to avoid curse of dimensionality of matching $s$ and $s_{obs}$
		\end{itemize}
		
		\vspace{0.3cm}
		$\Longrightarrow$ choose sufficient statistics, such that:
		\[   \pi(\theta|s_{obs}) \equiv \pi(\theta|y_{obs})   \]

	\end{frame}

	\begin{frame}{Distance measure}
		Distance measure: substantial impact on ABC algorithm efficiency
		
		\[  \parallel s - s_{obs} \parallel = (s - s_{obs})^\top \Sigma^{-1} (s - s_{obs}) \]
		
		\begin{itemize}
			\item $\Sigma = $ identity matrix $ \rightarrow$ Euclidean distance 
			\item $\Sigma =$ diagonal matrix of non-zero weights $ \rightarrow$ Weighted Euclidean distance 
			\item $\Sigma = $ full covariance matrix of $s \rightarrow$ Mahalanobis distance 
		\end{itemize}
	\end{frame}

	\begin{frame}{ABC rejection sampling Algorithm}

		\only<1> {
			The following is an ABC \textbf{algorithm}:
			\vspace{0.3cm}
			\emph{Inputs:}
			\begin{itemize}
				\item a target posterior density $\pi(\theta | y_{obs}) \propto p(y_{obs}|\theta) \pi(\theta)$, consisting of a prior distribution $\pi(\theta)$ and a procedure of generating data under the model $p(y_{obs}|\theta)$;
				\item a proposal density $g(\theta)$, with $g(\theta)\textgreater 0 $ if $\pi(\theta | y_{obs}) \textgreater 0$;
				\item an integer $N \textgreater0$;
				\item a kernel function $K_h(u)$ and a scale parameter $h > 0$;
				\item a low dimensional vector of summary statistics $s=S(y)$.
			\end{itemize}
		}
		\only<2> {
			\emph{Sampling} for $i= 1,..., N$:
			\begin{enumerate}
				\item generate $\theta ^ {(i)} \sim g(\theta)$ from sampling density $g$;
				\item generate $ y \sim p(y|\theta ^ {(i)})$ from the likelihood;
				\item compute summary statistic $s = S(y)$;
				\item accept $\theta ^ {(i)}$ with probability $\frac{K_h(\parallel s-s_{obs}\parallel)   \pi(\theta ^ {(i)})}{K g(\theta ^ {(i)})}$, where $K \geq K_h(0)\max_{\theta}{\frac{\pi(\theta)}{g(\theta)}}$; else go to 1.
			\end{enumerate}
			\emph{Output:}
			\begin{itemize}
				\item a set of parameter vectors $\theta ^ {(1)},..., \theta ^ {(N)} \sim \pi_{ABC}(\theta |S_{obs})$.
			\end{itemize}
		
		Possibly, can add a stopping rule.
		}
		

	\end{frame}
\end{section}

\begin{section}{Conclusions}
	
    \begin{frame}[plain]{}
		\sectionpage
	\end{frame}

	\begin{frame}{Next steps}
		Our focus till now was to understand the fundamental concepts and collect the missing information.
		
		\vspace{0.5 cm}
		The next step will be a \textbf{simple and separate implementation} of both solution to be tested on simulated data.
		
		\vspace{0.5 cm}
		Further steps will consider the \textbf{integration} of both solution into a single implementation and the testing on more complex data.
	\end{frame}

	\begin{frame}{Bibliography}
		\nocite{*}
		\bibliographystyle{unsrt}
		\tiny{ \bibliography{refs_MCMC,refs_ABC} }

	
%				\begin{minipage}[t]{0.4\textwidth}
%			\footnotesize {Unbiased Markov chain Monte Carlo methods with couplings:}
%			\tiny { \bibliography{refs_MCMC} }
%		\end{minipage}
%		\hfill
%		\begin{minipage}[t]{0.4\textwidth}
%			\footnotesize {Approximate Bayesian computation:}
%			\tiny { \bibliography{refs_ABC} }
%		\end{minipage}

	\end{frame}
\end{section}

\end{document}
